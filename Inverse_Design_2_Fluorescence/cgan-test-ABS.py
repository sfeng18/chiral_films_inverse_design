import time
import os
import torch
import torch.nn as nn
from torch.nn import functional as F
import spectrum as sp
import numpy as np
from torch.utils.data import DataLoader
import torch.utils.data
from torchvision import utils, datasets, transforms
import math
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from random import randint
from IPython.display import HTML
import matplotlib
from paras import  db_abs ,tag_wave, ideal_Gvalue

matplotlib.use('TkAgg')

batch_size = 16 # Batch size during training
num_epochs = 100  # Training times
lr = 0.001  # optimizer learning rate
beta1 = 0.9  # Beta1 hyperparameters of the Adam optimizer
ngpu = 1  # The number of available GPUs, use 0 to run in CPU mode.
# p = 0.00001  #Something like a learning rate in the discriminator
if db_abs == 'ABS':
    col_max, col_min = 1.806, 0.04
elif db_abs == 'PROP':
    col_max, col_min = 0.671413806451613, 0.07797111363636361
else:
    raise RuntimeError('ERROR: Unkown db_abs value: %s' % db_abs)
thick_max, thick_min = 80.0, 17.0
str_max, str_min = 267.0, 20.0
gra_max, gra_min = 8.0, 1.0
G_in_dim = 20

# torch.manual_seed(0)#Set a random seed to reproduce experimental results.
#Import Data

Color_Dict = {'蓝2': 'blue2', '蓝6': 'blue6', '蓝71': 'blue71', '刚果红': 'congo red', '绿': 'green', '橘红': 'orange', '紫': 'purple', '红': 'red', '台盼蓝': 'tai', '黄': 'yellow'}
Color_Values = ['yellow', 'orange', 'congo red', 'red', 'purple', 'tai', 'blue2', 'blue6', 'blue71', 'green']

NColors = 10
def minmaxscaler(data, max, min):  #MinMaxScaler normalization method
    a = (data - min) / (max - min)
    return a

def inverse_minmaxscaler(data, max, min):  #denormalization function
    # data=data.to(torch.float64)
    d = data * (max - min) + min
    return d


def getData1(sdb_statistics_file):  #read real data
    sdb_statistics = sp.read_file(sdb_statistics_file)
    a = {}  #store data
    b = {}  #store target
    realdata = []
    for index, cl in enumerate(Color_Values):
        x1 = []  #Store data of color, angle, thickness, degree of stretching, and grayscale
        for key in sdb_statistics.keys():
            sdb_statistics_key = key.split('-')
            if sdb_statistics_key[0] == cl:
                x1.append(index + 1)
                for j in range(3, 6):
                    x1.append(float(sdb_statistics_key[j]))
                for index1, value in enumerate(sdb_statistics[key]['X']):
                    if value == tag_wave:
                        k = index1
                x1.append(sdb_statistics[key]['G'][k])
        a[cl] = torch.from_numpy(np.array(x1).reshape(-1, 5))
        realdata.extend(x1)
    realdata = torch.from_numpy(np.array(realdata).reshape(-1, 5))
    return realdata

class Batch_Net(nn.Module):
    """
    On the basis of Activation_Net above, a method to speed up the convergence speed is added - batch normalization
    """

    def __init__(self, in_dim, n_hidden_1, n_hidden_2, n_hidden_3, n_hidden_4, n_hidden_5, out_dim):
        super(Batch_Net, self).__init__()
        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1), nn.BatchNorm1d(n_hidden_1), nn.ReLU(True))
        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2), nn.BatchNorm1d(n_hidden_2), nn.ReLU(True))
        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, n_hidden_3), nn.BatchNorm1d(n_hidden_3), nn.Dropout(p=0.5))
        self.layer4 = nn.Sequential(nn.Linear(n_hidden_3, n_hidden_4), nn.BatchNorm1d(n_hidden_4), nn.ReLU(True))
        self.layer5 = nn.Sequential(nn.Linear(n_hidden_4, n_hidden_5), nn.BatchNorm1d(n_hidden_5), nn.ReLU(True))
        self.layer6 = nn.Sequential(nn.Linear(n_hidden_5, out_dim))

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.layer5(x)
        x = self.layer6(x)
        return x


# Decide which device we run on
device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")
#join instantiation
Forward_net = Batch_Net(124, 200, 250, 250, 250, 200, 1)
#Extract the entire forward prediction network
Forward_net = torch.load('./Inverse_Design_2_Fluorescence/%g/net_predict_%s_%g.pkl' % (tag_wave,db_abs,tag_wave)).to(device)
Forward_net.eval()


#network
#In the discrimination part, it is judged whether the data generated by the generator conforms to the range, and if so, it is thrown into the fully connected prediction network to predict the G value; if not, a false G value is given
def distinguish(input_data):
    data = torch.empty_like(input_data)
    real_input = torch.empty(batch_size, 124)
    for j in range(0, batch_size):
        data[j][0] = inverse_minmaxscaler(input_data[j][0], NColors, 1)
        data[j][1] = inverse_minmaxscaler(input_data[j][1], thick_max, thick_min)
        data[j][2] = inverse_minmaxscaler(input_data[j][2], str_max, str_min)
        data[j][3] = inverse_minmaxscaler(input_data[j][3], gra_max, gra_min)
        complete_data = data_transform(data[j])
        complete_data_norm = norm(complete_data)
        real_input[j] = torch.squeeze(complete_data_norm)
    return Forward_net(real_input.to(device))


def my_loss(input_data):#Change here as needed
    err = torch.sum(F.relu(-input_data) + F.relu(input_data - 1))+ torch.sum(torch.abs(input_data[:, 0] - 5 / 9.0))+ torch.sum(torch.abs(input_data[:, 1] - 63 / 63.0))
    return err


#weight initialization
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)


#build generator
class Generator(nn.Module):

    def __init__(self, ngpu):
        self.ngpu = ngpu
        super(Generator, self).__init__()
        self.data = nn.Sequential(
            nn.Linear(G_in_dim, 40),
            nn.LazyBatchNorm1d(),
            nn.ReLU(True),
            nn.Linear(40, 80),
            nn.LazyBatchNorm1d(),
            nn.ReLU(True),
            nn.Linear(80, 20),
            nn.LazyBatchNorm1d(),
            nn.ReLU(True),
            nn.Linear(20, 5),
        )

    def forward(self, x):
        x = self.data(x)
        return x


# create generator
netG = Generator(ngpu).to(device)

#If you want to use multiple GPUs, set it.
if (device.type == 'cuda') and (ngpu > 1):
    netG = nn.DataParallel(netG, list(range(ngpu)))

# Use the weight initialization function weights_init to randomly initialize all weights
#  mean=0, stdev=0.2.
netG.apply(weights_init)

#read data
realdata = getData1('./Data/sdb.msgz')
data0 = realdata[:,:-1]
dataloader = torch.utils.data.DataLoader(dataset=realdata, batch_size=batch_size, shuffle=True, drop_last=True)
#Optimizer and loss function
# Initialize BCELoss function  
criterion = nn.MSELoss(reduction='mean')

# Setup Adam optimizers for both G and D
optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))

if db_abs == 'ABS':
    def colorMatrix(dye_property_file):
        dye_property = sp.read_file(dye_property_file)
        color_matrix = []  #Store 121 parameters corresponding to the color
        j = 1
        for cl in Color_Values:
            color_info = []  #Store 121 pigment characteristic absorption spectrum data
            color_last = len(dye_property[cl]['abs'])  #Pick out 121 data
            color_step = int(color_last / 120)
            for i in range(0, color_last, color_step):  #Different colors have different amounts of data
                color_info.append(dye_property[cl]['abs'][i])
            color_matrix.append(color_info)
            j += 1
        return torch.tensor(color_matrix, dtype=torch.float32)

    color_matrix = colorMatrix('./Data/dye_ABS.msgz').to(device)
else:
    def colorMatrix(dye_property_file):
        dye_property = sp.read_file(dye_property_file)
        color_matrix = []  #Store 121 parameters corresponding to the color
        j = 1
        for cl in Color_Values:
            color_info = []  #Store 121 pigment characteristic absorption spectrum data
            color_last = len(dye_property[cl])  #Pick out 121 data
            color_step = int(color_last / 120)
            for i in range(0, color_last, color_step):  #Different colors have different amounts of data
                color_info.append(dye_property[cl][i][1])
            color_matrix.append(color_info)
            j += 1
        return torch.tensor(color_matrix, dtype=torch.float32)

    color_matrix = colorMatrix('./Data/dye_prop.msgz').to(device)


#Color data changed from 1 to 121
def color_transform2(color):
    color_labels = torch.from_numpy(np.arange(1.0, NColors + 1.0, dtype=np.float32)).to(device)
    color_distance = F.relu(1.0 - torch.abs(color_labels - color))
    color_onehot = F.softmax(color_distance * 20, 0)
    return torch.matmul(color_onehot.reshape(1, -1), color_matrix)


#The total data changed from 4 to 124, and normalized at the same time
def data_transform(data):
    clist = torch.squeeze(color_transform2(data[0]))
    data1 = torch.flatten(torch.cat((clist, data[1:]), 0))
    return data1


def norm(data):
    #Globally normalize colors
    color_x = data[:-3]  #One-dimensional color data
    color_x = minmaxscaler(color_x, col_max, col_min)
    color_x = torch.unsqueeze(color_x, 0)
    #Column-wise normalization for the other three parameters
    thick_x = minmaxscaler(data[-3], thick_max, thick_min)
    thick_x = torch.unsqueeze(torch.unsqueeze(thick_x, 0), 0)
    str_x = minmaxscaler(data[-2], str_max, str_min)
    str_x = torch.unsqueeze(torch.unsqueeze(str_x, 0), 0)
    gra_x = minmaxscaler(data[-1], gra_max, gra_min)
    gra_x = torch.unsqueeze(torch.unsqueeze(gra_x, 0), 0)
    x_un = torch.cat([color_x, thick_x, str_x, gra_x], dim=1)
    return x_un


#train
# Lists to keep track of progress  
img_list = []
G_losses = []
D_losses = []
D_x_list = []
D_z_list = []
# loss_tep = 10

print("Starting Pre Training Loop...")
# For each epoch 

for i, data in enumerate(dataloader, 0):
    optimizerG.zero_grad()
    real_G = data[:, -1]
    b_size = real_G.size(0)
    noise = torch.randn(b_size, G_in_dim, device=device)

    fake = netG(noise)
    # Calculate G's loss based on this output   
    err_New = my_loss(fake[:, :-1])
    err_New.backward()
    # Update G  
    optimizerG.step()
    # Output training stats   
    print(f'Step: [{i+1:0>{len(str(len(dataloader)))}}/{len(dataloader)}]', f'Loss-New: {err_New.item():.4f}', end='\r')

print("Starting Real Training Loop...")
for epoch in range(num_epochs):

    beg_time = time.time()
    # For each batch in the dataloader 
    for i, data in enumerate(dataloader, 0):

        optimizerG.zero_grad()

        real_G = data[:, -1]
        b_size = real_G.size(0)
        noise = torch.randn(b_size, G_in_dim, device=device)
        fake = netG(noise)
        input_D = fake[:, :-1]
        generate_G = fake[:, -1]
        # Since we just updated D, perform another forward pass of all-fake batch through D
       
        output = distinguish(input_D)
        distinguish_G = output[:, -1]
        ideal_G = torch.full((b_size,), ideal_Gvalue, device=device)
        ideal_G = ideal_G.to(torch.float32)
        # Calculate G's loss based on this output  
        err_New = my_loss(input_D)
        err_New.backward(retain_graph=True)
        err_GG = criterion(ideal_G, distinguish_G)
        err_GG.backward(retain_graph=True)
        # err_GG.backward()
        err_G2 = criterion(generate_G, ideal_G)
        # err_G2=err_G2.to(torch.float32)
        err_G2.backward()
        # Calculate gradients for G 
        D_G_z = output.mean().item()
        # Update G 
        optimizerG.step()
        # Output training stats   
        end_time = time.time()
        run_time = round(end_time - beg_time)
        print(
            f'Epoch: [{epoch+1:0>{len(str(num_epochs))}}/{num_epochs}]',
            f'Step: [{i+1:0>{len(str(len(dataloader)))}}/{len(dataloader)}]',
            f'Loss-New: {err_New.item():.4f}',
            f'Loss-GG: {err_GG.item():.4f}',
            f'Loss-G2: {err_G2.item():.4f}',
            f'G(z): {D_G_z:.4f}',
            # f'D(G(z)): [{D_G_z1:.4f}/{D_G_z:.4f}]',
            f'Time: {run_time}s',
            end='\r')

        # Save Losses for plotting later 
        G_losses.append(err_GG.item())
        D_losses.append(err_G2.item())
        D_z_list.append(D_G_z)

# Create batch of latent vectors and laebls that we will use to visualize the progression of the generator
fixed_noise = torch.randn(100, G_in_dim).to(device)
plt.figure(figsize=(10, 5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses, label="G")
plt.plot(D_losses, label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

torch.save(netG, './Inverse_Design_2_Fluorescence/%g/netG_%s_%g_%g.pkl' % (tag_wave, db_abs, tag_wave, ideal_Gvalue))


def data_in_range(d):
    return (NColors >= d[0] >= 1 and thick_max >= d[1] >= thick_min and str_max >= d[2] >= str_min and gra_max >= d[3] >= gra_min)


def data_real(d):
    if abs(d[0] - round(float(d[0]))) > 0.3:
        return False
    for thick in (17, 30, 48, 60, 80):
        if abs(d[1] - thick) < 3:
            return True
    return False


# Get a batch of real images from the dataloader
real_batch = next(iter(dataloader))
noise1 = torch.randn(5000, G_in_dim, device=device)
result_data = netG(noise1)
print(result_data)
result_size = result_data.size(0)
right = []
net_G_data = []
loss_G_data = []
gan_G_data = []
OutFile = './Inverse_Design_2_Fluorescence/%g/cgan_test_result_%s_%g_%g.csv' % (tag_wave, db_abs, tag_wave,ideal_Gvalue)
for j in range(0, result_size):
    result_data[j][0] = inverse_minmaxscaler(result_data[j][0], NColors, 1)
    result_data[j][1] = inverse_minmaxscaler(result_data[j][1], thick_max, thick_min)
    result_data[j][2] = inverse_minmaxscaler(result_data[j][2], str_max, str_min)
    result_data[j][3] = inverse_minmaxscaler(result_data[j][3], gra_max, gra_min)
    # if result_data[j][4].to(torch.float64)>=-0.6:
    if data_in_range(result_data[j]) and data_real(result_data[j]):
        print(' '.join(['%g' % _ for _ in result_data[j]]))
        gan_G_data.append(result_data[j][4].item())
        complete_data = data_transform(result_data[j][:-1])
        complete_data_norm = norm(complete_data)
        net_G = Forward_net(complete_data_norm)
        net_G_data.append(net_G.item())
        loss_G = net_G - result_data[j][4]
        loss_G_data.append(loss_G.item())
        print('神经网络预测：' + str(net_G.item()) + '\n', 'gan网络预测：' + str(result_data[j][4].item()) + '\n', '两网络差：' + str(loss_G.item()) + '\n')
        if abs(loss_G)<0.1:
            if not os.path.isfile(OutFile):
                Head = 'Color,Thick,Strain,Gray,G_GAN,G_Predict,dG\n'
                open(OutFile, 'wt').writelines(Head)
            with open(OutFile, 'a') as f:
                f.write(','.join(['%g'%_ for _ in result_data[j].tolist()[:4] + [result_data[j][4].item(), net_G.item(), loss_G.item()]]) + '\n')

plt.plot(net_G_data, 'b')
plt.plot(gan_G_data, 'g')
plt.plot(loss_G_data, 'r')
plt.show()
